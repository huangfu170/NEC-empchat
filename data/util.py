# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import argparse
import collections
import json
import logging
import math
import os
import sys
from typing import List

import torch
import tqdm
from torch.nn.utils.rnn import pad_sequence
from torch import logsumexp
from transformers import AutoTokenizer, AutoModel


from metrics.distinct.distinct import Distinct
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
from nltk import word_tokenize
from torch.nn import functional as F
from dataclasses import dataclass
from transformers.utils import ModelOutput
from typing import Optional, Tuple

@dataclass
class BeamSearchConfig:
    beam_size: int
    num_return_sequences: int
    max_length: int
    min_length: int
    early_stopping: bool
    length_penalty: float
@dataclass
class EncoderOutputs(ModelOutput):
    x_encoded: torch.FloatTensor=None
    social_knowledge_encoder_outputs: Optional[torch.FloatTensor] = None
    
@dataclass
class EncoderMask(ModelOutput):
    x_mask: torch.FloatTensor=None
    social_knowledge_mask: Optional[torch.FloatTensor] = None

@dataclass
class Seq2SeqModelOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
    encoder_last_hidden_state: Optional[torch.FloatTensor] = None
    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
    emotion_loss: Optional[torch.FloatTensor] = None
def _force_token_ids_generation( scores, token_id) -> None:
    """force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float("inf"))"""
    scores[:, [x for x in range(scores.size(-1)) if x != token_id]] = -float("inf")

def adjust_logits_during_generation(cur_len, logits, bos_token_id,eos_token_id,max_length):
    if cur_len == 1:
        _force_token_ids_generation(logits, bos_token_id)
    elif cur_len == max_length - 1 and eos_token_id is not None:
        _force_token_ids_generation(logits, eos_token_id)
    return logits
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
@torch.no_grad()
def get_similarities(model, source_sentence:str,candidate_sentences:List[str], tokenizer, device, need_embedding=False):
    # Tokenize sentences
    source_input=tokenizer(source_sentence, padding=True, truncation=True, return_tensors='pt')
    cand_input = tokenizer(candidate_sentences, padding=True, truncation=True, return_tensors='pt')
    source_input=source_input.to(device)
    cand_input=cand_input.to(device)
    # Compute token embeddings
    source_output = model(**source_input)
    candidate_output = model(**cand_input)

    # Perform pooling
    source_embeddings = mean_pooling(source_output, source_input['attention_mask'])
    sentence_embeddings = mean_pooling(candidate_output, cand_input['attention_mask'])

    # Normalize embeddings
    source_embeddings = F.normalize(source_embeddings, dim=1)
    sentence_embeddings = F.normalize(sentence_embeddings,dim=1)
    score=[F.cosine_similarity(source_embeddings,sentence_embeddings[i].unsqueeze(0),dim=1).item() for i in range(len(candidate_sentences))]
    score=torch.tensor(score)
    score,indices=torch.topk(score,1)
    if need_embedding:
        return sentence_embeddings,score.tolist(),indices.tolist()
    return score.tolist(),indices.tolist()

def get_sentence_similarity_model(path):
    tokenizer = AutoTokenizer.from_pretrained(path)
    model = AutoModel.from_pretrained(path)
    return {
        'tokenizer': tokenizer,
        'model': model
    }
def get_entity_knowl(data, rel_list):
    res = []
    scores = []
    for t in rel_list:
        res.append([])
        assert t in data.keys()
        for i in range(data[t].size(0)):
            res[-1].append(data[t][i])
        label_k = f'{t}_label'
        scores.append(data[label_k])

    res = [pad_sequence(res[i], batch_first=True, padding_value=1) for i in range(len(res))] if res else None
    res = torch.cat(res, dim=1) if res else None

    # size:(四种常识所含的实体数，最长的用于评分的长度)
    scores = torch.cat(scores, dim=0) if scores else None
    return res, scores
def get_knowledge(data, rel_list):
    t = []
    for reltype in rel_list:
        assert reltype in data.keys()
        t.append(data[reltype])
    if len(t) > 1:
        res = []
        for i in range(len(rel_list)):
            res.extend([t[i][j] for j in range(t[i].size(0))])
        res = pad_sequence(res, batch_first=True, padding_value=1)
        return res
    else:
        return t[0]
def get_score(model,tokenizer,source,candidate):
    return get_similarities(model, source,candidate, tokenizer, 'cuda:0')


def count_dataset(dataset):
    Counter=collections.Counter()
    import re
    split_pattern=r'[,.!?，。！？]'
    for data in dataset:
        uttr=data['response']
        uttr=re.split(split_pattern,uttr)
        uttr=[item.strip() for item in uttr if len(item.strip())>0]
        Counter.update(uttr)
    return Counter


def greedy_search(model, _input, _msk, _question, eos_token_id, bos_token_id, max_len=20):
    bs = _input.size(0)
    device = _input.device
    end_tokens = [False for _ in range(bs)]
    res = torch.ones(bs, 1, dtype=torch.long, device=device) * bos_token_id
    pred_outputs = torch.ones(bs, _input.size(-1) + max_len, dtype=torch.long, device=device) * eos_token_id
    pred_outputs[:, :_input.size(-1)] = _input
    with torch.no_grad():
        for k in range(max_len):
            logits = model(_input, _msk, res, _question)[-1]
            logits = logits[:, -1, :]
            pred = torch.argmax(logits, dim=-1)
            for b in range(bs):
                if pred[b].item() == eos_token_id:
                    end_tokens[b] = True
                if not end_tokens[b]:
                    pred_outputs[b, _input.size(-1) + k] = pred[b]

            res = torch.cat([res, pred.unsqueeze(1)], dim=-1)

    return pred_outputs


class BeamHypotheses(object):
    def __init__(self, num_beams, max_length, length_penalty, early_stopping):
        """
        Initialize n-best list of hypotheses.
        """
        self.max_length = max_length - 1  # ignoring bos_token
        self.length_penalty = length_penalty
        self.early_stopping = early_stopping
        self.num_beams = num_beams
        self.beams = []
        self.worst_score = 1e9

    def __len__(self):
        """
        Number of hypotheses in the list.
        """
        return len(self.beams)

    def add(self, hyp, sum_logprobs,beam_indices=None):
        """
        Add a new hypothesis to the list.
        """
        score = sum_logprobs / len(hyp) ** self.length_penalty
        if len(self) < self.num_beams or score > self.worst_score:
            self.beams.append((score, hyp,beam_indices))
            if len(self) > self.num_beams:
                #当前存的已经超过num_beams，也就是加入新的之后为beam width+1
                sorted_scores = sorted([(s, idx) for idx, (s, _ , _) in enumerate(self.beams)]) #默认按照tuple第一索引排序
                del self.beams[sorted_scores[0][1]] # 最低的分数对应的id
                self.worst_score = sorted_scores[1][0] # 更新最低分数
            else:
                self.worst_score = min(score, self.worst_score)

    def is_done(self, best_sum_logprobs, cur_len):
        """
        If there are enough hypotheses and that none of the hypotheses being generated
        can become better than the worst one in the heap, then we are done with this sentence.
        """

        if len(self) < self.num_beams:
            return False
        elif self.early_stopping:
            return True
        else:
            cur_score = best_sum_logprobs / cur_len ** self.length_penalty
            ret = self.worst_score >= cur_score # 现在best_score对应的概率还比不上worst_score
            return ret

@torch.no_grad()
def beam_search(model, input_ids, attention_mask, social_knowledge=None, social_knowledge_mask=None,emotion=None, beam_width=5, pad_token_id=1, decoder_bos_id=2, eos_token_id=2, bos_token_id=0, max_len=30,num_return_sequences=1):

    assert beam_width>=num_return_sequences, "beam_width must be greater than num_return_sequences"


    bs=input_ids.size(0)
    device=input_ids.device

    input_ids=input_ids.unsqueeze(1).repeat(1,beam_width,1).view(bs*beam_width,-1)
    attention_mask=attention_mask.unsqueeze(1).repeat(1,beam_width,1).view(bs*beam_width,-1)
    if emotion is not None:
        emotion=emotion.unsqueeze(1).repeat(1,beam_width).view(bs*beam_width)
    # social knowledge : [batch size, knowledge num, knowledge len]
    if social_knowledge is not None:
        knowledge_num=social_knowledge.size(1)
        social_knowledge=social_knowledge.unsqueeze(1).repeat(1,beam_width,1,1).view(bs*beam_width,knowledge_num,-1)
    # social_knowledge_mask : [batch size, knowledge num, knowledge len]
    if social_knowledge is not None:
        social_knowledge_mask=social_knowledge_mask.unsqueeze(1).repeat(1,beam_width,1,1).view(bs*beam_width,knowledge_num,-1)
      

    if hasattr(model.model,"get_encoder_outputs"):
        encoder_outputs=model.model.get_encoder_outputs(input_ids,attention_mask,social_knowledge,social_knowledge_mask)

    generated_hyps = [
        BeamHypotheses(beam_width, max_len, 1, early_stopping=False)
        for _ in range(bs)
    ]
    decoder_input_ids = torch.full((bs * beam_width, 1), decoder_bos_id, dtype=torch.long, device=device)

    done= [False for _ in range(bs)]
    beam_indices = (
            tuple(() for _ in range(bs*beam_width))
    )
    decoder_hidden_states=()
    scores=()
    beam_scores = torch.zeros((bs, beam_width), dtype=torch.float, device=device)
    beam_scores[:, 1:] = -1e9
    beam_scores = beam_scores.view(-1)
    cur_len=1
    while cur_len<max_len:

        input_={
            "input_ids":input_ids,
            "attention_mask":attention_mask,
            "decoder_input_ids":decoder_input_ids,
            "social_knowledge":social_knowledge,
            "social_knowledge_mask":social_knowledge_mask,
            "encoder_outputs":encoder_outputs,
            "emotion":emotion
        }

        outputs=model(**input_)
        next_token_logits=outputs['lm_logits'][:,-1,:] #(bs*beam,vocab_size)
        rows_to_select = []
        for i in range(bs):
            rows_to_select.extend([i * beam_width + j for j in range(num_return_sequences)])
        rows_to_select= torch.tensor(rows_to_select, dtype=torch.long, device=next_token_logits.device)
        if 'decoder_hidden_states' in outputs:  
            decoder_hidden_states+=(outputs['decoder_hidden_states'][rows_to_select][:,-1,:].unsqueeze(1),)

        next_token_logits=adjust_logits_during_generation(cur_len,next_token_logits,bos_token_id,eos_token_id,max_len)
        next_token_logits=F.log_softmax(next_token_logits,dim=-1) #(bs*beam_width,vocab_size)

        vocab_size=next_token_logits.size(-1)

        # topk
        next_scores=next_token_logits+beam_scores.unsqueeze(-1)
        scores+=(next_scores,)
        next_scores=next_scores.view(
            bs,beam_width*vocab_size
        )
        next_scores,next_tokens=torch.topk(next_scores,2*beam_width,dim=1,largest=True,sorted=True) #(bs,2*bw)

        next_batch_beam=[]
        for batch_idx in range(bs):
            if done[batch_idx]:
                next_batch_beam.extend([(0,pad_token_id,0)]*beam_width) #这个batch如果已经结束了，那么就补pad
                continue
            
            next_sent_beam=[]
            # 检查这个batch_idx的所有候选token
            for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(
                        zip(next_tokens[batch_idx], next_scores[batch_idx])
            ):
                # beam_token_rank : 当前这个token_id和score的在beam中的排序位置
                beam_id = beam_token_id // vocab_size
                token_id = beam_token_id % vocab_size

                effective_beam_id = batch_idx * beam_width + beam_id

                if (eos_token_id is not None) and (token_id.item() == eos_token_id):
                    # 如果当前预测的词是eos_id
                    is_beam_token_worse_than_top_width_beams = beam_token_rank >= beam_width 
                    # 如果eos_token_id对应的词其实在beam_width之后，则句子结束的置信度比较低
                    # 需要判断它在词表概率里的位置，如果大于beam width表示其实并不在我们的正常beam width的选择范围内,
                    # 那么跳过这个eos，换别的词
                    if is_beam_token_worse_than_top_width_beams:
                        continue
                    beam_index = beam_indices[effective_beam_id] + (effective_beam_id,) if beam_indices else None

                    # 合理的句子结束
                    generated_hyps[batch_idx].add(
                        decoder_input_ids[effective_beam_id].clone(),
                        beam_token_score.item(),
                        beam_indices=beam_index,
                    )
                else:   
                    # add next predicted token since it is not eos_token
                    next_sent_beam.append((beam_token_score, token_id, effective_beam_id))

                if len(next_sent_beam) == beam_width:
                    break
            # 当前sentence的状态如果原来为结束就无所谓，如果是原来不是结束，那么就要判断
            # 当前sentence的最大概率词是不是比历史最低的概率还要低，如果是的话则直接结束
            # 出于可信度的判断
            done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(
                next_scores[batch_idx].max().item(), cur_len
            )
            
            assert len(next_sent_beam) == beam_width, "Beam should always be full"
            next_batch_beam.extend(next_sent_beam)
            assert len(next_batch_beam) == beam_width * (batch_idx + 1), "We should have added num_beams each step"
            
        if all(done):
            break

        assert len(next_batch_beam) == bs * beam_width #一个token处理结束
        # new function creates a tensor with device same as called variable
        beam_scores = beam_scores.new([x[0] for x in next_batch_beam]) # beam_token_score
        beam_tokens = decoder_input_ids.new([x[1] for x in next_batch_beam]) # token_id 这轮的token
        beam_idx = decoder_input_ids.new([x[2] for x in next_batch_beam]) # effective_beam_id
        beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))

        decoder_input_ids = decoder_input_ids[beam_idx, :]
        decoder_input_ids = torch.cat([decoder_input_ids, beam_tokens.unsqueeze(1)], dim=-1)
        cur_len = cur_len + 1


    for batch_idx in range(bs):
        if done[batch_idx]:
            continue
        if eos_token_id is not None and all(
            (token_id % vocab_size).item() != eos_token_id for token_id in next_tokens[batch_idx]
        ):
            #如果现在所有预测词都不是eos
            assert torch.all(
                next_scores[batch_idx, :beam_width] == beam_scores.view(bs, beam_width)[batch_idx]
            ), "If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}".format(
                next_scores[:, :beam_width][batch_idx],
                beam_scores.view(bs, beam_width)[batch_idx],
            )

        # need to add best beam_width hypotheses to generated hyps
        for beam_id in range(beam_width):
            effective_beam_id = batch_idx * beam_width + beam_id
            final_score = beam_scores[effective_beam_id].item()
            final_tokens = decoder_input_ids[effective_beam_id]
            beam_index = beam_indices[effective_beam_id] if beam_indices is not None else None
            generated_hyps[batch_idx].add(final_tokens, final_score,beam_indices=beam_index)
    
    output_batch_size = bs
    output_num_return_sequences_per_batch = num_return_sequences

    sent_lengths=decoder_input_ids.new(output_batch_size*output_num_return_sequences_per_batch) #在当前decode step后的各句子长度
    best=[] #用来存当前decode step 对应的hyp
    best_indices=[]
    best_scores=torch.zeros(bs * beam_width, device=device)
    for i,hypotheses in enumerate(generated_hyps):
        sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])
        for j in range(output_num_return_sequences_per_batch):
            effective_batch_idx = output_num_return_sequences_per_batch * i + j

            best_hyp_tuple=sorted_hyps.pop()
            best_score=best_hyp_tuple[0]
            best_hyp = best_hyp_tuple[1]# 当前的best句子
            best_index=best_hyp_tuple[2]

            sent_lengths[effective_batch_idx] = len(best_hyp)
            best.append(best_hyp)
            best_indices.append(best_index)

            best_scores[effective_batch_idx] = best_score

    # prepare for adding eos
    sent_max_len = min(sent_lengths.max().item() + 1, max_len)
    decoded = decoder_input_ids.new(output_batch_size*output_num_return_sequences_per_batch, sent_max_len)

    if len(best_indices) > 0 and best_indices[0] is not None:
        indices: torch.LongTensor = input_ids.new(bs * output_num_return_sequences_per_batch, sent_max_len)
    else:
        indices = None

    if indices is not None:
        indices.fill_(-1)

    # shorter batches are padded if needed
    if sent_lengths.min().item() != sent_lengths.max().item():
        assert pad_token_id is not None, "`pad_token_id` has to be defined"
        decoded.fill_(pad_token_id)

    # fill with hypotheses and eos_token_id if the latter fits in
    for i, (hypo,best_idx) in enumerate(zip(best,best_indices)):
        decoded[i, : sent_lengths[i]] = hypo

        if indices is not None:
            indices[i,:len(best_idx)]=torch.tensor(best_idx)

        if sent_lengths[i] < max_len:
            decoded[i, sent_lengths[i]] = eos_token_id
    # decode:(bs*bw,seq_len)
    return {
        "sequences":decoded,
        "beam_indices":indices,
        "encoder_hidden_states":encoder_outputs[0][torch.arange(0,bs*beam_width,beam_width).long()],
        "decoder_hidden_states":decoder_hidden_states,
        "sequences_scores":best_scores,
    }





def calc_corpus_bleu_new(hypothesis, references):
    # hypothesis = [normalize_answer(hyp).split(" ") for hyp in hypothesis]
    # references = [[normalize_answer(ref).split(" ")] for ref in references]
    references = [[gold] for gold in references]
    sf = SmoothingFunction(epsilon=1e-12).method1
    b1 = corpus_bleu(references, hypothesis, weights=(1.0/1.0,), smoothing_function=sf)
    b2 = corpus_bleu(references, hypothesis, weights=(1.0/2.0, 1.0/2.0), smoothing_function=sf)
    b3 = corpus_bleu(references, hypothesis, weights=(1.0/3.0, 1.0/3.0, 1.0/3.0), smoothing_function=sf)
    b4 = corpus_bleu(references, hypothesis, weights=(1.0/4.0, 1.0/4.0, 1.0/4.0, 1.0/4.0), smoothing_function=sf)
    return {
        "bleu1":b1,
        "bleu2":b2,
        "bleu3":b3,
        "bleu4":b4
    }

def compute_metrics(eval_pred, model_name, pad_token_id, epoch,need_print=True):
    '''

    :param eval_pred: Tuple(label,logits) size:(BS,seq_len)
    :return:
    '''

    predictions, labels, = eval_pred

    tokenizer, vocab_size = get_tokenizer(model_name)
    pred=[torch.tensor(i) for i in predictions]
    distinct = Distinct(3)
    

    pred = pad_sequence(pred, True, pad_token_id)
    labels, predictions = tokenizer.batch_decode(labels, skip_special_tokens=True), tokenizer.batch_decode(pred,
                                                                                                           skip_special_tokens=True)
    predictions=predictions[::len(predictions)//len(labels)]
    tlabels,tpredictions=[word_tokenize(l.strip()) for l in labels],[word_tokenize(p.strip()) for p in predictions]
    print(tpredictions[:10]) #打印前十个看看

    # print("-----------------预测---------------------------")
    if need_print:
        with open("epoch-{}.txt".format(epoch), "w") as f:
            for i in range(len(labels)):
                if len(predictions[i]) < 4:
                    # 这里为什么需要做str(j)??
                    f.write("".join([str(j) for j in labels[i]]) + "\t" + "Prediction 长度小于4" + "".join(
                        [str(j) for j in predictions[i]]) + "\n")
                else:
                    f.write(
                        "".join([str(j) for j in labels[i]]) + "\t||\t" + "".join([str(j) for j in predictions[i]]) + "\n")

    bleu=calc_corpus_bleu_new(tpredictions,tlabels)
    res={}
    res.update(bleu)
    res.update(distinct.compute(tpredictions,epoch,False))
    return res


def generate_outputs(model,dataset,dataloader,tokenizer,file_path,args=None):
    assert not os.path.exists(file_path), "file path {} already exists".format(file_path)

    device= model.device
    res=[]
    gr=[]
    for data in dataset:
        gr.append(data['response'])
    rel_list = ['xReact', 'xNeed', 'xIntent', 'xEffect', 'xWant']
    entity_rel_list = ['ObjectUse', 'AtLocation', 'MadeUpOf', 'HasProperty']

    with torch.no_grad():
        for data in tqdm.tqdm(dataloader):
            model.eval()
            _msk = data['attention_mask'].to(device)
            _input, _response = data['input_ids'].to(device), data['response'].to(device)
            _knowledge = get_knowledge(data, rel_list).to(device)
            _emotion = data['emotion'].to(device)

            if 'ObjectUse' in data:
                _entity_knowledge, scores = get_entity_knowl(data, entity_rel_list)
                if _entity_knowledge is not None and scores is not None:
                    _entity_knowledge = _entity_knowledge.to(device)
                _entity_knowledge_mask = (_entity_knowledge != 1).float().to(device)
                # Prompt
                prompt = torch.tensor([0, 133, 511, 2655, 4905, 32, 2200, 4249, 7, 5, 314, 25860, 35, 2]).unsqueeze(
                    0).repeat(_entity_knowledge.size(0), 1).to(device)
                prompt2 = torch.tensor([0, 48360, 35, 2]).unsqueeze(
                    0).repeat(_entity_knowledge.size(0), 1).to(device)
                _input = torch.cat([prompt2, _input, prompt, _entity_knowledge], dim=1)
                _msk = torch.cat([torch.ones_like(prompt2), _msk, torch.ones_like(prompt), _entity_knowledge_mask],
                                 dim=1)
            else:
                _entity_knowledge = None
                _entity_knowledge_mask = None
                scores = None
            # pred = beam_search(model, _input, _msk, _knowledge, None, _emotion, 5, 1, 2, 2,
            #                    0)['sequences'].tolist()
            pred=model.generate(_input, _msk, _knowledge, None, 5, args).tolist()
            pred = tokenizer.batch_decode(pred, skip_special_tokens=True)
            res.extend(pred)

    with open(file_path, "w") as f:
        for i in range(len(res)):
            f.write("Pred:"+"".join([str(j) for j in res[i]])+"\n")
            f.write("GR:" + "".join([str(j) for j in gr[i]]) + "\n")



def seed_everything(seed: int):
    import random, os
    import numpy as np
    import torch

    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def unzip_batch(data, device):
    return [
        field.to(device)
        if field is not None
        else None
        for field in data
    ]
