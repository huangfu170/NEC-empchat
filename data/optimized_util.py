# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
# Optimized version of beam search implementation

import torch
import torch.nn.functional as F
from typing import List, Optional, Tuple, Union
from torch import logsumexp


def _force_token_ids_generation(scores, token_id) -> None:
    """Force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float("inf"))"""
    scores[:, [x for x in range(scores.size(-1)) if x != token_id]] = -float("inf")


def adjust_logits_during_generation(cur_len, logits, bos_token_id, eos_token_id, max_length):
    """Optimized logits adjustment with in-place operations"""
    if cur_len == 1:
        _force_token_ids_generation(logits, bos_token_id)
    elif cur_len == max_length - 1 and eos_token_id is not None:
        _force_token_ids_generation(logits, eos_token_id)
    return logits


class OptimizedBeamHypotheses:
    """Optimized beam hypotheses management with better memory efficiency"""
    
    def __init__(self, num_beams: int, max_length: int, length_penalty: float, early_stopping: bool):
        self.max_length = max_length - 1
        self.length_penalty = length_penalty
        self.early_stopping = early_stopping
        self.num_beams = num_beams
        self.beams = []
        self.worst_score = 1e9

    def __len__(self):
        return len(self.beams)

    def add(self, hyp: torch.Tensor, sum_logprobs: float, beam_indices: Optional[Tuple] = None):
        """Optimized add with better score calculation"""
        score = sum_logprobs / (len(hyp) ** self.length_penalty)
        
        if len(self) < self.num_beams or score > self.worst_score:
            self.beams.append((score, hyp, beam_indices))
            
            if len(self) > self.num_beams:
                # Use more efficient sorting and removal
                self.beams.sort(key=lambda x: x[0])
                self.beams.pop(0)  # Remove worst (first after sorting)
                self.worst_score = self.beams[0][0]  # New worst score
            else:
                self.worst_score = min(score, self.worst_score)

    def is_done(self, best_sum_logprobs: float, cur_len: int) -> bool:
        """Early stopping check"""
        if len(self) < self.num_beams:
            return False
        elif self.early_stopping:
            return True
        else:
            cur_score = best_sum_logprobs / (cur_len ** self.length_penalty)
            return self.worst_score >= cur_score


@torch.no_grad()
def optimized_beam_search(
    model,
    _input: torch.Tensor,
    _msk: torch.Tensor,
    _knowl: Optional[torch.Tensor] = None,
    _emotion: Optional[torch.Tensor] = None,
    beam_width: int = 5,
    pad_token_id: int = 1,
    decoder_bos_id: int = 2,
    eos_token_id: int = 2,
    bos_token_id: int = 0,
    max_len: int = 30,
    num_return_sequences: int = 1,
    _entity_know: Optional[torch.Tensor] = None
):
    """
    Optimized beam search implementation with improved memory efficiency and performance.
    
    Key optimizations:
    1. Reduced memory allocations through in-place operations
    2. Vectorized tensor operations where possible
    3. More efficient beam management
    4. Better memory layout for tensor operations
    """
    bs = _input.size(0)
    device = _input.device
    
    # Pre-allocate tensors to avoid repeated allocations
    decoder_input_ids = torch.full((bs * beam_width, 1), decoder_bos_id, 
                                 dtype=torch.long, device=device)
    
    # Initialize beam hypotheses
    generated_hyps = [
        OptimizedBeamHypotheses(beam_width, max_len, 1.0, early_stopping=False)
        for _ in range(bs)
    ]
    
    # Optimized tensor expansion - use repeat instead of expand for better memory layout
    _input_expanded = _input.unsqueeze(1).repeat(1, beam_width, 1).view(bs * beam_width, -1)
    _msk_expanded = _msk.unsqueeze(1).repeat(1, beam_width, 1).view(bs * beam_width, -1)
    
    # Handle optional inputs more efficiently
    _emotion_expanded = None
    if _emotion is not None:
        _emotion_expanded = _emotion.unsqueeze(1).repeat(1, beam_width).view(bs * beam_width)
    
    _knowl_expanded = None
    _knowl_mask = None
    if _knowl is not None:
        _knowl_expanded = _knowl.unsqueeze(1).repeat(1, beam_width, 1).view(bs * beam_width, -1)
        _knowl_mask = (_knowl_expanded != 1).float()
    
    _entity_know_expanded = None
    _entity_know_mask = None
    if _entity_know is not None:
        _entity_know_expanded = _entity_know.unsqueeze(1).repeat(1, beam_width, 1).view(bs * beam_width, -1)
        _entity_know_mask = (_entity_know_expanded != 1).float()
    
    # Pre-compute encoder outputs if available
    encoder_outputs = []
    if hasattr(model.model, "get_encoder_outputs"):
        x_encoded, x_knowl, x_entity = model.model.get_encoder_outputs(
            _input_expanded, _msk_expanded, _knowl_expanded, _knowl_mask,
            entity_knowledge=_entity_know_expanded, 
            entity_knowledge_mask=_entity_know_mask
        )
        encoder_outputs.extend([x_encoded, x_knowl, x_entity])
    
    # Initialize beam tracking
    done = [False] * bs
    beam_indices = tuple(() for _ in range(bs * beam_width))
    decoder_hidden_states = ()
    scores = ()
    
    # Initialize beam scores more efficiently
    beam_scores = torch.zeros(bs * beam_width, dtype=torch.float, device=device)
    beam_scores[beam_width::beam_width] = -1e9  # Set non-first beams to low score
    
    cur_len = 1
    
    # Main generation loop
    while cur_len < max_len:
        # Prepare input dictionary
        input_dict = {
            "input_ids": _input_expanded,
            "attention_mask": _msk_expanded,
            "decoder_input_ids": decoder_input_ids,
            "knowl": _knowl_expanded,
            "knowl_mask": _knowl_mask,
            "encoder_outputs": encoder_outputs,
            "emotion": _emotion_expanded
        }
        
        # Forward pass
        outputs = model(**input_dict)
        next_token_logits = outputs['lm_logits'][:, -1, :]  # (bs*beam, vocab_size)
        
        # Optimized row selection
        rows_to_select = torch.arange(bs, device=device).unsqueeze(1) * beam_width + \
                        torch.arange(num_return_sequences, device=device).unsqueeze(0)
        rows_to_select = rows_to_select.view(-1)
        
        # Store decoder hidden states more efficiently
        if 'decoder_hidden_states' in outputs:
            decoder_hidden_states += (outputs['decoder_hidden_states'][rows_to_select][:, -1, :].unsqueeze(1),)
        
        # Apply logits adjustment
        next_token_logits = adjust_logits_during_generation(
            cur_len, next_token_logits, bos_token_id, eos_token_id, max_len
        )
        next_token_logits = F.log_softmax(next_token_logits, dim=-1)
        
        vocab_size = next_token_logits.size(-1)
        
        # Optimized score calculation
        next_scores = next_token_logits + beam_scores.unsqueeze(-1)
        scores += (next_scores,)
        
        # Reshape for topk operation
        next_scores_reshaped = next_scores.view(bs, beam_width * vocab_size)
        next_scores_topk, next_tokens = torch.topk(
            next_scores_reshaped, 2 * beam_width, dim=1, largest=True, sorted=True
        )
        
        # Process beams more efficiently
        next_batch_beam = []
        for batch_idx in range(bs):
            if done[batch_idx]:
                next_batch_beam.extend([(0, pad_token_id, 0)] * beam_width)
                continue
            
            next_sent_beam = []
            for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(
                zip(next_tokens[batch_idx], next_scores_topk[batch_idx])
            ):
                beam_id = beam_token_id // vocab_size
                token_id = beam_token_id % vocab_size
                effective_beam_id = batch_idx * beam_width + beam_id
                
                if (eos_token_id is not None) and (token_id.item() == eos_token_id):
                    # Handle EOS token
                    if beam_token_rank >= beam_width:
                        continue
                    
                    beam_index = beam_indices[effective_beam_id] + (effective_beam_id,) if beam_indices else None
                    generated_hyps[batch_idx].add(
                        decoder_input_ids[effective_beam_id].clone(),
                        beam_token_score.item(),
                        beam_indices=beam_index,
                    )
                else:
                    # Add next predicted token
                    next_sent_beam.append((beam_token_score, token_id, effective_beam_id))
                
                if len(next_sent_beam) == beam_width:
                    break
            
            # Check if done
            done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(
                next_scores_topk[batch_idx].max().item(), cur_len
            )
            
            next_batch_beam.extend(next_sent_beam)
        
        if all(done):
            break
        
        # Update beam states more efficiently
        beam_scores = torch.tensor([x[0] for x in next_batch_beam], device=device)
        beam_tokens = torch.tensor([x[1] for x in next_batch_beam], device=device)
        beam_idx = torch.tensor([x[2] for x in next_batch_beam], device=device)
        
        # Update beam indices
        beam_indices = tuple(
            beam_indices[beam_idx[i]] + (beam_idx[i],) 
            for i in range(len(beam_indices))
        )
        
        # Update decoder input
        decoder_input_ids = decoder_input_ids[beam_idx]
        decoder_input_ids = torch.cat([decoder_input_ids, beam_tokens.unsqueeze(1)], dim=-1)
        cur_len += 1
    
    # Final processing - add remaining hypotheses
    for batch_idx in range(bs):
        if done[batch_idx]:
            continue
        
        for beam_id in range(beam_width):
            effective_beam_id = batch_idx * beam_width + beam_id
            final_score = beam_scores[effective_beam_id].item()
            final_tokens = decoder_input_ids[effective_beam_id]
            beam_index = beam_indices[effective_beam_id] if beam_indices else None
            generated_hyps[batch_idx].add(final_tokens, final_score, beam_indices=beam_index)
    
    # Prepare output
    output_batch_size = bs
    output_num_return_sequences_per_batch = num_return_sequences
    
    # Collect best hypotheses
    sent_lengths = torch.zeros(output_batch_size * output_num_return_sequences_per_batch, 
                              dtype=torch.long, device=device)
    best = []
    best_indices = []
    best_scores = torch.zeros(bs * beam_width, device=device)
    
    for i, hypotheses in enumerate(generated_hyps):
        sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])
        for j in range(output_num_return_sequences_per_batch):
            effective_batch_idx = output_num_return_sequences_per_batch * i + j
            
            if sorted_hyps:
                best_hyp_tuple = sorted_hyps.pop()
                best_score, best_hyp, best_index = best_hyp_tuple
                
                sent_lengths[effective_batch_idx] = len(best_hyp)
                best.append(best_hyp)
                best_indices.append(best_index)
                best_scores[effective_batch_idx] = best_score
            else:
                # Handle case where no hypotheses were generated
                sent_lengths[effective_batch_idx] = 1
                best.append(torch.tensor([eos_token_id], device=device))
                best_indices.append(None)
                best_scores[effective_batch_idx] = -1e9
    
    # Prepare final output tensors
    sent_max_len = min(sent_lengths.max().item() + 1, max_len)
    decoded = torch.full((output_batch_size * output_num_return_sequences_per_batch, sent_max_len),
                        pad_token_id, dtype=torch.long, device=device)
    
    # Fill with hypotheses
    for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):
        if len(hypo) > 0:
            decoded[i, :sent_lengths[i]] = hypo
            if sent_lengths[i] < max_len:
                decoded[i, sent_lengths[i]] = eos_token_id
    
    # Prepare beam indices if needed
    indices = None
    if len(best_indices) > 0 and best_indices[0] is not None:
        indices = torch.full((bs * output_num_return_sequences_per_batch, sent_max_len), 
                           -1, dtype=torch.long, device=device)
        for i, best_idx in enumerate(best_indices):
            if best_idx is not None:
                indices[i, :len(best_idx)] = torch.tensor(best_idx, device=device)
    
    return {
        "sequences": decoded,
        "beam_indices": indices,
        "encoder_hidden_states": encoder_outputs[0][torch.arange(0, bs * beam_width, beam_width).long()] if encoder_outputs else None,
        "decoder_hidden_states": decoder_hidden_states,
        "sequences_scores": best_scores,
    }


# Additional utility functions for performance comparison
def benchmark_beam_search(original_func, optimized_func, model, input_data, num_runs=10):
    """Benchmark original vs optimized beam search"""
    import time
    
    # Warm up
    for _ in range(3):
        _ = original_func(model, **input_data)
        _ = optimized_func(model, **input_data)
    
    # Benchmark original
    start_time = time.time()
    for _ in range(num_runs):
        _ = original_func(model, **input_data)
    original_time = time.time() - start_time
    
    # Benchmark optimized
    start_time = time.time()
    for _ in range(num_runs):
        _ = optimized_func(model, **input_data)
    optimized_time = time.time() - start_time
    
    print(f"Original beam search: {original_time:.4f}s")
    print(f"Optimized beam search: {optimized_time:.4f}s")
    print(f"Speedup: {original_time/optimized_time:.2f}x")
    
    return original_time, optimized_time


def memory_usage_analysis(func, model, input_data):
    """Analyze memory usage of beam search functions"""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    result = func(model, **input_data)
    
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_used = memory_after - memory_before
    
    print(f"Memory usage: {memory_used:.2f} MB")
    return memory_used, result
